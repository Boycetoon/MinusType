---
layout: post
title: "使用PSPNet对街景图像进行语义分割"
categories:
  - 数字辅助城市设计
tags:
  - 街景地图
  - Python
  - 图像分割
  - 深度学习
---
回来填坑了~~

本文算是之前街景图像获取的后续，城市研究中使用街景图像来进行物体识别与语义分割可以说是最常见的应用，当然现在这个方面可以说是很成熟了，作为非人工智能算法研究领域的使用者完全没有必要造轮子。之前我自己处理数据也是使用的姚尧博士分享的封装好的FCN模型（在没有特殊的科研需要的情况下完全够用）。

不过最近看李小江博士的分享说是采用的PSPNet进行的语义分割，于是借此机会做了些尝试，并且配合之前的街景图像获取的流程进行了完善。对于这个方法而言，预训练的模型完全是现成的，相对的优势可能在于**支持GPU训练提高速度**以及**和街景获取部分结合形成完整的工作流**。

## 在Cityscapes数据集上预训练的PSPNet模型

**Pyramid Scene Parsing Network（PSPNet）**是CVPR2017上关于场景解析的文章，拿到了2016年ImageNet比赛中scene parsing任务的冠军，当然也常用来做语义分割。PSPNet算法是目前应用比较广泛的语义分割算法之一，该算法在PASCAL VOC2012测试集上的mIOU是82.6%。当然我对算法的了解也仅限于读过论文原文，感兴趣的话可以看原文链接：https://arxiv.org/abs/1612.01105。

这里使用的是GluonCV中的预训练模型，在Cityscapes数据集上进行了训练，Cityscapes是重要的城市街道图像分割数据集，更加适合用来进行街景图像的语义分割。
## 预先准备
- 由于使用的是GluonCV，所以需要预先安装GluonCV及其依赖的MXNet，可以参考官网给出的安装方式进行安装：[https://cv.gluon.ai/install.html](https://cv.gluon.ai/install.html)
- MXNet具有CPU版本和GPU版本，在深度学习计算中使用GPU计算可以极大地提高速度，但安装GPU版本前需要安装CUDA以及cuDNN，可以参考：[https://zhuanlan.zhihu.com/p/91554516](https://zhuanlan.zhihu.com/p/91554516)
- 上述安装过程中需要Microsoft Visual C++ 2015以上版本，具体安装地址可以在上一条的教程中找到。
- 代码部分是按照GPU训练编写的，如果安装的CPU版本则需要进行一定的修改。
## 街景图像批量语义分割

首先对图像进行批量的语义分割，并将结果存储到本地csv表格中。这里使用的文件结构与命名法将与之前街景获取的教程一致，图像的命名方式为 ID_Lng_Lat_Heading.jpg，比如“61.0_126.6849976_45.7885017_180.jpg”。

```python
# 导入需要使用的包
import os
import mxnet as mx
from mxnet import image, gpu
import gluoncv
from gluoncv.data.transforms.presets.segmentation import test_transform
from gluoncv.utils.viz import get_color_pallete,plot_image
import matplotlib.pyplot as plt
import matplotlib.image as mpimg
import numpy as np
import pandas as pd

# 忽略警告
import warnings; warnings.filterwarnings(action='once') 
warnings.filterwarnings("ignore")

# 设定使用GPU或者CUP进行计算，没有安装GPU版本的MXNet请使用CPU
ctx = mx.gpu(0)

# 定义函数对单张图片进行图像分割，并将结果存为pd.Series
def get_seg(file, model):
  img = image.imread(file)
  img = test_transform(img,ctx=ctx)
  output = model.predict(img)
  predict = mx.nd.squeeze(mx.nd.argmax(output, 1)).asnumpy()
  # 定义Cityscapes数据集分割标签字典
  col_map = {0:'road', 1:'sidewalk', 2:'building', 3:'wall', 4:'fence', 5:'pole', 6:'traffic light',
             7:'traffic sign', 8:'vegetation', 9:'terrain', 10:'sky', 11:'person', 12:'rider',
             13:'car', 14:'truck', 15:'bus', 16:'train', 17:'motorcycle', 18:'bicycle'}
  pred = []
  for i in range(19):
      pred.append((len(predict[predict==i])/(predict.shape[0]*predict.shape[1])))
  pred = pd.Series(pred).rename(col_map)
  return pred

# 下载模型，这里使用的是在Cityscapes数据集上预训练的PSPnet模型
model = gluoncv.model_zoo.get_model('psp_resnet101_citys', ctx=ctx, pretrained=True)

file_path = r'.' # 传入保存图片的文件夹路径
filelist = os.listdir(file_path)
df = pd.DataFrame(columns=['id','lng','lat','heading','road','sidewalk','building','wall','fence',
                         'pole','traffic light','traffic sign','vegetation','terrain','sky',
                         'person','rider','car','truck','bus','train','motorcycle','bicycle'])
for i in filelist: # 循环遍历所有的图片进行语义分割，并将结果存如pd.DataFrame
  img_path = file_path + "/" + i
  try:
      img_id = float(i[:-4].split('_')[0])
  except:
      continue
  else:
      lng = float(i[:-4].split('_')[1])
      lat = float(i[:-4].split('_')[2])
      heading = float(i[:-4].split('_')[3])
      data_i = pd.Series({'id':img_id, 'lng':lng, 'lat':lat, 'heading':heading}).append(get_seg(img_path, model))
      df = pd.concat([df, pd.DataFrame(data_i).T], axis=0, join='outer', ignore_index=True)
      print('---------图片' + i + '已完成分割计算--------')

df.to_csv(file_path + "/img_seg.csv") # 将结果保存到csv
```
## 后续处理与可视化

本来做到这一步就可以导入GIS进行下一步计算，但是还并不算完善，得到的数据中并没有对每个点的四个方向的值进行汇总平均，并且借助geopandas包可以将数据转换为shp格式，以及进行可视化。所以后面的部分算是可选内容。
### 分割结果可视化

首先是对图像分割的结果进行可视化：

```python
# 对单张图片进行可视化
img_path = r'.\176.0_126.6819992_45.7871017_180.jpg'
img = image.imread(img_path)
img = test_transform(img,ctx=ctx)
output = model.predict(img)
predict = mx.nd.squeeze(mx.nd.argmax(output, 1)).asnumpy()

mask = get_color_pallete(predict, 'citys')
base = mpimg.imread(img_path)
plt.figure(figsize=(10,5))
plt.imshow(base)
plt.imshow(mask,alpha=0.5)
plt.axis('off')
#plt.savefig(".",dpi=150,bbox_inches='tight')
```

得到针对单张分割结果的颜色遮罩：
### 数据汇总并转为shp

第二步是将单个点的四个方向的数据进行汇总平均，并且存储为shp格式文件。不过我的geopandas和mxnet不在一个环境，所以这里切换了kernel并且重新导入了一次包。

```python
# 对得到的数据进行处理并保存为shp
# 由于geopandas和mxnet不在同一个环境，所以这里切换kernel重新导入一次包
import numpy as np
import pandas as pd
import geopandas as gpd
import matplotlib.pyplot as plt
from shapely.geometry import Point

csv_path = r'.\img_seg.csv'
data = pd.read_csv(csv_path)
data = data.groupby('id').mean().reset_index().drop(['Unnamed: 0','heading'],axis=1) #
data['geometry'] = data.apply(lambda x: Point(x['lng'], x['lat']), axis=1)
geo_data = gpd.GeoDataFrame(data)

geo_data.to_file('./img_seg.shp', encoding='utf-8')
```

最后将得到如下结构的shp文件：
### 对绿视率进行可视化

最后一步本来之前是在GIS里可视化的，不过既然都转成GeoDataFrame格式了也可以在Python里进行。这里使用了点的颜色表达绿视率，而使用点的大小表达建筑比率。由于这次运行的数据只是之前街景的一小部分，整体图像只有一小块，但是在更大量数据下一样适用。这里导入了小旭学长提供的plot_map包来下载OSM底图，可以参考：https://gitee.com/ni1o1/pygeo-tutorial。同时也推荐小旭学长的这一套教程，非常实用~

```python
# 对点数据进行可视化，这里使用了颜色代表绿视率，点的大小代表建筑比率
import matplotlib as mpl
import plot_map # 导入同济小旭学长提供的底图绘制包

fig     = plt.figure(1,(10,6))    
ax      = plt.subplot(111)
plt.sca(ax)

bounds = [126.65,45.78,126.69,45.79]
plot_map.plot_map(plt,bounds,zoom=18,style=4)
vmax = geo_data['vegetation'].max()
norm = mpl.colors.Normalize(vmin=0,vmax=vmax)
cmapname = 'autumn_r'
cmap = mpl.cm.get_cmap(cmapname)

color= cmap(norm(geo_data['vegetation']))
size = 200*(geo_data['building']-min(geo_data['building']))/(max(geo_data['building'])-min(geo_data['building']))
plt.scatter(geo_data['lng'], geo_data['lat'], c=color, s=size, alpha=0.5)

plt.axis('off')
ax.set_xlim(bounds[0], bounds[2])
ax.set_ylim(bounds[1], bounds[3])
plt.show()
```

结果如下：